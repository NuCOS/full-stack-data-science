{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ML App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a machine learning model to predict the defaults, let us try to build a web application to lend loans. \n",
    "\n",
    "It'll have two parts: \n",
    "\n",
    "* a form to submit the loans\n",
    "* admin panel to look at the submitted loans and their probability of defaults\n",
    "\n",
    "The source code for the ML app is available in the github repo in `credit-risk/webap` folder.\n",
    "It has all the moving parts except integration with the model.\n",
    "\n",
    "Start the application using:\n",
    "\n",
    "    python webapp.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML as a Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While we can package the model with the webapp and use it, it created tight coupling between the two. Everytime the model changes, the webapp will have to change. What if there are more than one application using the same model? \n",
    "\n",
    "It is lot simpler to deploy the ML model as a service exposing it's functionality through an HTTP API.\n",
    "\n",
    "In this turorial we are going to use a tool called firefly for running the model as a service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Firefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firefly makes it very easy to deploy functions as a service without having to worry about writing a web app, managing request/response formats etc. and also provides a very simple client interface.\n",
    "\n",
    "The detailed documentation of Firefly is available at:\n",
    "\n",
    "<http://firefly-python.readthedocs.io/>\n",
    "\n",
    "Let's try a simple example. Here We're creating a file `sq.py` with a square function. We'll see how deploy it a service and use in other programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file sq.py\n",
    "\n",
    "def square(n):\n",
    "    return n*n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run it as a service using firefly by running the following command in your terminal.\n",
    "\n",
    "    $ firefly sq.square\n",
    "    [2017-07-13 09:48:07 +0200] [5001] [INFO] Starting gunicorn 19.7.1\n",
    "    [2017-07-13 09:48:07 +0200] [5001] [INFO] Listening at: http://127.0.0.1:8000 (5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes the `<module_name>.<function_name>` as argument and exposes the function as an API. The argument the function takes and the return value must be JSON-friendly for it work.\n",
    "\n",
    "Once that is running, we can try to access it using the firefly client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import firefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote_sq = firefly.Client(\"http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_sq.square(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function will be available with the same name in the client. Please note that the client functions takes parameters only by name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run on a different port, you can specify that as an argument.\n",
    "\n",
    "    $ firefly -b 0.0.0.0:9000 sq.square\n",
    "\n",
    "For more help on the available command-line options, try:\n",
    "    \n",
    "    $ firefly --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Write funciton `add_numbers` in a file `add.py` and deploy it as a service using Firefly. Once that is ready, try to use it in another program using the Firefly Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file add.py\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Grade Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banks will have a access to the credit grade of each customer. Since we haven't have real data, let us build a simple mock credit grade service.\n",
    "\n",
    "It'll take the email address of the person and gives a grade at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file credit_grade.py\n",
    "\"\"\"Program to find the credit grade of a person.\n",
    "\"\"\"\n",
    "import zlib\n",
    "import random\n",
    "\n",
    "def find_credit_grade(email):\n",
    "    \"\"\"Returns the credit grade of the person identified by the given email address.\n",
    "    \n",
    "    The credit grade can be either A, B, C, D, E, F or G.\n",
    "    \"\"\"\n",
    "    # since we need to give the same grade everytime the function is called\n",
    "    # with the same email. Using the checksum of the string as random seed \n",
    "    # to get the same result everytime when used with the same email.\n",
    "    seed = zlib.adler32(email.encode(\"utf-8\"))\n",
    "    r = random.Random(seed)\n",
    "    return r.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy it as a servive using Firefly.\n",
    "\n",
    "    firefly credit_grade.find_credit_grade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_grade_api = firefly.Client(\"http://127.0.0.1:8000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_grade_api.find_credit_grade(email=\"alice@example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the machine learning model as a service, we need to read the model and all the encodings that we have used in building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting credit_risk_service.py\n"
     ]
    }
   ],
   "source": [
    "%%file credit_risk_service.py\n",
    "\"\"\"Service to expose the credit risk model as an API.\n",
    "\"\"\"\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# read the encoders and the model\n",
    "grade_encoder = joblib.load(\"../notebooks/le_grade.pkl\")\n",
    "ownership_encoder = joblib.load(\"../notebooks/le_ownership.pkl\")\n",
    "model = joblib.load(\"../notebooks/model.pkl\")\n",
    "\n",
    "def predict(amount, years, age, ownership, income, grade):\n",
    "    \"\"\"Returns the probablity of default for given features.\n",
    "    \"\"\"\n",
    "    # encoders work on a vector. Wrapping in a list as we only have a single value\n",
    "    ownership_code = ownership_encoder.transform([ownership])[0]\n",
    "    grade_code = grade_encoder.transform([grade])[0]\n",
    "    \n",
    "    # important to pass the features in the same order as we built the model\n",
    "    features = [amount, grade_code, years, ownership_code, income, age]\n",
    "    \n",
    "    # probablity for not-defaulting and defaulting\n",
    "    # Again, wrapping in a list as a list of features is expected\n",
    "    p0, p1 = model.predict_proba([features])[0]\n",
    "    return p1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it as a service using firefly, again from your terminal. Let us use port 9000 now as port 8000 is used by the credit grade service.\n",
    "\n",
    "    $ firefly -b 127.0.0.1:9000 credit_risk_service.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firefly\n",
    "credit_risk_api = firefly.Client(\"http://127.0.0.1:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_risk_api.predict(amount=10000, \n",
    "                        years=2, \n",
    "                        age=35, \n",
    "                        ownership='RENT', \n",
    "                        income=12345, \n",
    "                        grade='A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you try to integrate the credit grade service and credit risk prediction service into the our webapp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
